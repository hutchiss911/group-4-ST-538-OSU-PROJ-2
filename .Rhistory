max.depth = 6, eta = 0.01, nrounds = 100,
nfold = 5, early_stopping_rounds = 5,
objective = "binary:logistic", eval_metric = "logloss")
sel_rounds = xgb.loan.cv$best_iteration
xgb.loan2 = xgboost(data = train.data, label = train.labels,
max.depth = 6, eta = 1, nrounds = sel_rounds,
objective = "binary:logistic", eval_metric = "logloss")
# Fitted response labels
pred1 = predict(xgb.loan, train.data)
pred.xgb.loan = ifelse(pred1 > 0.5, 2, 1)
pred2 = predict(xgb.loan2, train.data)
pred.xgb.loan2 = ifelse(pred2 > 0.5, 2, 1)
# Plotting the first two trees in the boosted classifier
xgb.plot.tree(model = xgb.loan, trees = 1:2)
# Misclassification errors in the training data
train_conf_matrix1 <- table(filteredData$LOAN_DEFAULT[train_indices], pred.xgb.loan)
train_conf_matrix2 <- table(filteredData$LOAN_DEFAULT[train_indices], pred.xgb.loan2)
#Accuracy
train_accuracy1 <- (train_conf_matrix1[1,1] + train_conf_matrix1[2,2]) / sum(train_conf_matrix1)
precision1 <- train_conf_matrix1[2, 2] / sum(train_conf_matrix1[, 2])
recall1 <- train_conf_matrix1[2, 2] / sum(train_conf_matrix1[2, ])
train_accuracy1
precision1
recall1
train_accuracy2 <- (train_conf_matrix2[1,1] + train_conf_matrix2[2,2]) / sum(train_conf_matrix2)
precision2 <- train_conf_matrix2[2, 2] / sum(train_conf_matrix2[, 2])
recall2 <- train_conf_matrix2[2, 2] / sum(train_conf_matrix2[2, ])
train_accuracy2
precision2
recall2
set.seed(1243)
n <- dim(filteredData)[1]
trainValIndices <- sample(1:n, size = round(0.8 * n))  # 80% for training and validation
testIndices <- setdiff(1:n, train_val_indices)  # Remaining 20% for testing
set.seed(1243)
n <- dim(filteredData)[1]
trainValIndices <- sample(1:n, size = round(0.8 * n))  # 80% for training and validation
testIndices <- setdiff(1:n, trainValIndices)  # Remaining 20% for testing
#splitting the training dataset into a new training and validation set
trainIndices <- sample(trainValIndices, size = round(0.75 * length(trainValIndices)))  # 75% - training
valIndices <- setdiff(trainValIndices, trainIndices)  # Remaining 25% of train_val_indices for validation
train2.data <- (filteredData[trainIndices, 1:16])
val2.data <- (filteredData[valIndices, 1:16])
test2.data <- (filteredData[testIndices, 1:16])
train2.labels <- as.numeric(filteredData$LOAN_DEFAULT[trainIndices])
val2.labels <- as.numeric(filteredData$LOAN_DEFAULT[valIndices])
test2.labels <- as.numeric(filteredData$LOAN_DEFAULT[testIndices])
set.seed(1243)
n <- dim(filteredData)[1]
trainValIndices <- sample(1:n, size = round(0.8 * n))  # 80% for training and validation
testIndices <- setdiff(1:n, trainValIndices)  # Remaining 20% for testing
#splitting the training dataset into a new training and validation set
trainIndices <- sample(trainValIndices, size = round(0.8 * length(trainValIndices)))  # 75% - training
valIndices <- setdiff(trainValIndices, trainIndices)  # Remaining 25% of train_val_indices for validation
train2.data <- (filteredData[trainIndices, 1:16])
val2.data <- (filteredData[valIndices, 1:16])
test2.data <- (filteredData[testIndices, 1:16])
train2.labels <- as.numeric(filteredData$LOAN_DEFAULT[trainIndices])
val2.labels <- as.numeric(filteredData$LOAN_DEFAULT[valIndices])
test2.labels <- as.numeric(filteredData$LOAN_DEFAULT[testIndices])
set.seed(1243)
n <- dim(filteredData)[1]
trainValIndices <- sample(1:n, size = round(0.8 * n))  # 80% for training and validation
testIndices <- setdiff(1:n, trainValIndices)  # Remaining 20% for testing
#splitting the training dataset into a new training and validation set
trainIndices <- sample(trainValIndices, size = round(0.8 * length(trainValIndices)))  # 80% - training
valIndices <- setdiff(trainValIndices, trainIndices)  #  20% - validation
train2.data <- (filteredData[trainIndices, 1:16])
val2.data <- (filteredData[valIndices, 1:16])
test2.data <- (filteredData[testIndices, 1:16])
train2.labels <- as.numeric(filteredData$LOAN_DEFAULT[trainIndices])
val2.labels <- as.numeric(filteredData$LOAN_DEFAULT[valIndices])
test2.labels <- as.numeric(filteredData$LOAN_DEFAULT[testIndices])
set.seed(1243)
n <- dim(filteredData)[1]
trainValIndices <- sample(1:n, size = round(0.8 * n))
testIndices <- setdiff(1:n, trainValIndices)
#splitting the training dataset into a new training and validation set
trainIndices <- sample(trainValIndices, size = round(0.8 * length(trainValIndices)))  # 80% - training
valIndices <- setdiff(trainValIndices, trainIndices)  #  20% - validation
train2.data <- (filteredData[trainIndices, 1:16])
val2.data <- (filteredData[valIndices, 1:16])
test2.data <- (filteredData[testIndices, 1:16])
train2.labels <- as.numeric(filteredData$LOAN_DEFAULT[trainIndices])
val2.labels <- as.numeric(filteredData$LOAN_DEFAULT[valIndices])
test2.labels <- as.numeric(filteredData$LOAN_DEFAULT[testIndices])
set.seed(1243)
n <- dim(filteredData)[1]
trainValIndices <- sample(1:n, size = round(0.8 * n))
testIndices <- setdiff(1:n, trainValIndices)
#splitting the training dataset into a new training and validation set
trainIndices <- sample(trainValIndices, size = round(0.8 * length(trainValIndices)))  # 80% - training
valIndices <- setdiff(trainValIndices, trainIndices)  #  20% - validation
train2.data <- (filteredData[trainIndices, 1:16])
val.data <- (filteredData[valIndices, 1:16])
train2.labels <- as.numeric(filteredData$LOAN_DEFAULT[trainIndices])
val.labels <- as.numeric(filteredData$LOAN_DEFAULT[valIndices])
set.seed(1243)
n <- dim(filteredData)[1]
trainValIndices <- sample(1:n, size = round(0.8 * n))
testIndices <- setdiff(1:n, trainValIndices)
#splitting the training dataset into a new training and validation set
trainIndices <- sample(trainValIndices, size = round(0.8 * length(trainValIndices)))  # 80% - training
valIndices <- setdiff(trainValIndices, trainIndices)  #  20% - validation
train2.data <- (filteredData[trainIndices, 1:16])
val.data <- (filteredData[valIndices, 1:16])
train2.labels <- as.numeric(filteredData$LOAN_DEFAULT[trainIndices])
val.labels <- as.numeric(filteredData$LOAN_DEFAULT[valIndices])
train2.data <- as.matrix(train.data[ , 1:15])
val.data <- as.matrix(val.data[ , 1:15])
# XGBoost Classifier
xgb.loan <- xgboost(
data = train.data, label = train.labels,
max.depth = 6, eta = 0.01, nrounds = 100, #using the same eta from last time for now.
objective = "binary:logistic", eval_metric = "logloss",
verbose = 0
)
#predictions
predTrain2 <- predict(xgb.loan, train.data)
predTrainClass2 <- ifelse(pred_train > 0.5, 1, 0)
train2.data <- as.matrix(train.data[ , 1:15])
val.data <- as.matrix(val.data[ , 1:15])
# XGBoost Classifier
xgb.loan <- xgboost(
data = train.data, label = train.labels,
max.depth = 6, eta = 0.01, nrounds = 100, #using the same eta from last time for now.
objective = "binary:logistic", eval_metric = "logloss",
verbose = 0
)
#predictions
predTrain2 <- predict(xgb.loan, train.data)
predTrainClass2 <- ifelse(predTrain2 > 0.5, 1, 0)
#Evaluate the performance
trainAccuracy3 <- mean(predTrainClass2 == train2.labels)
cat("Current Accuracy (mean): ", trainAccuracy3)
len(predTrainClass2)
length(predTrainClass2)
length(train2.labels)
train2.data <- as.matrix(train.data[ , 1:15])
val.data <- as.matrix(val.data[ , 1:15])
# XGBoost Classifier
xgb.loan <- xgboost(
data = train2.data, label = train.labels,
max.depth = 6, eta = 0.01, nrounds = 100, #using the same eta from last time for now.
objective = "binary:logistic", eval_metric = "logloss",
verbose = 0
)
#predictions
predTrain2 <- predict(xgb.loan, train.data)
predTrainClass2 <- ifelse(predTrain2 > 0.5, 1, 0)
#Evaluate the performance
trainAccuracy3 <- mean(predTrainClass2 == train2.labels)
cat("Current Accuracy (mean): ", trainAccuracy3)
train.data[ , 1:16]
train.data[1:16]
train.data[,1:16]
length(predTrainClass2)
length(train2.labels)
length(train2.data)
train2.data <- as.matrix(train.data[ , 1:16])
train2.data <- as.matrix(train.data[ , 1:16])
train2.data <- as.matrix(train.data[ , 0:15])
train2.data <- as.matrix(train.data[ , 0:15])
val.data <- as.matrix(val.data[ , 0:15])
# XGBoost Classifier
xgb.loan <- xgboost(
data = train2.data, label = train.labels,
max.depth = 6, eta = 0.01, nrounds = 100, #using the same eta from last time for now.
objective = "binary:logistic", eval_metric = "logloss",
verbose = 0
)
#predictions
predTrain2 <- predict(xgb.loan, train.data)
predTrainClass2 <- ifelse(predTrain2 > 0.5, 1, 0)
#Evaluate the performance
trainAccuracy3 <- mean(predTrainClass2 == train2.labels)
cat("Current Accuracy (mean): ", trainAccuracy3)
##An if statement for checking if a package is installed
if (!require(tidycensus)) {
install.packages("tidycensus")
}
if (!require(tidyverse)) {
install.packages("tidyverse")
}
if (!require(dplyr)) {
install.packages("dplyr")
}
if (!require(ggplot2)) {
install.packages("ggplot2")
}
if (!require(caret)) {
install.packages("caret")
}
if (!require(xgboost)) {
install.packages("xgboost")
}
#if (!require(DiagrammeR)) {
#install.packages("htmltools")
#}
knitr::opts_chunk$set(echo=F, error=T, collapse=T, warning=F, results='hide', message=F)
#Load libraries
library(tidycensus)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(caret)
library(xgboost)
#library(DiagrammeR)
#First, we'll import the dataset from the training CSV:
# Reading in the csv
train <- read.csv('train.csv')
train <- train %>% select(-"DISBURSAL_DATE")
train$DATE_OF_BIRTH <- as.Date(train$DATE_OF_BIRTH, format = "%d-%m-%Y")
train$AGE <- as.Date('01-01-2019', format = "%d-%m-%Y") - train$DATE_OF_BIRTH
train$AGE <- as.integer(floor(train$AGE / 365.25))
train <- train %>% select(-"DATE_OF_BIRTH", -"PERFORM_CNS_SCORE_DESCRIPTION")
#We then scrub the unknown values in our length and age fields:
# Calculate strange data fields
train$acctyr <- as.numeric(gsub("yrs.*", "", train$AVERAGE_ACCT_AGE))
train$acctmo <- as.numeric(gsub(".*yrs|mon", "", train$AVERAGE_ACCT_AGE))
train$crdtyr <- as.numeric(gsub("yrs.*", "", train$CREDIT_HISTORY_LENGTH))
train$crdtmo <- as.numeric(gsub(".*yrs|mon", "", train$CREDIT_HISTORY_LENGTH))
#Then we do our calculations and create various other fields for our modeling usage:
# Replace strange data fields
train$AVERAGE_ACCT_AGE <- round(train$acctyr + train$acctmo / 12, 2)
train$CREDIT_HISTORY_LENGTH <- round(train$crdtyr + train$crdtmo / 12, 2)
# Remove calc fields
train <- train %>% select(-acctyr, -acctmo, -crdtyr, -crdtmo)
# Create emloyment dummy fields
train$SELF_EMPLOYED <- ifelse(train$EMPLOYMENT_TYPE == "Self employed", 1, 0)
train$SALARIED <- ifelse(train$EMPLOYMENT_TYPE == "Salaried", 1, 0)
train$NULL_EMPLOYMENT <- ifelse(is.na(train$EMPLOYMENT_TYPE), 1, 0)
# Remove employment_type & Unique ID
train <- train %>% select(-EMPLOYMENT_TYPE, -UNIQUEID)
# Pull CNS score letter grade, removed to use XGBoost for now since it only takes integer/numbers
#train$PERFORM_CNS_SCORE_DESCRIPTION <- as.factor(substr(train$PERFORM_CNS_SCORE_DESCRIPTION,
# Remove rows with any null values
train <- train[complete.cases(train), ]
# Remove duplicate rows
train <- train[!duplicated(train), ]
# converting Loan Default to a factor for binary classification
tempTrain <- train
tempTrain$LOAN_DEFAULT <- as.factor(train$LOAN_DEFAULT)
# Convert all columns to numeric
tempTrain[] <- lapply(tempTrain, as.numeric)
# Check for NaN or infinite values in the correlation matrix and replace with 0
correlation_matrix <- cor(tempTrain)
correlation_matrix <- as.matrix(correlation_matrix)
correlation_matrix[is.nan(correlation_matrix) | is.infinite(correlation_matrix)] <- 0
# Define the plot margins to accommodate all text labels
par(mar = c(5, 5, 4, 2) + 0.1)
# Plot correlation matrix directly without clustering with variable labels
image(1:nrow(correlation_matrix), 1:ncol(correlation_matrix), correlation_matrix,
main = "Correlation Matrix Heatmap",
xlab = "",
ylab = "",
col = colorRampPalette(c("blue", "white", "red"))(100),
axes = FALSE)
# Add labels to the axes with smaller font size and remove numbers
axis(1, at = 1:ncol(correlation_matrix), labels = colnames(correlation_matrix), las = 2, cex.axis = 0.5, tck = 0)
axis(2, at = 1:nrow(correlation_matrix), labels = rownames(correlation_matrix), las = 2, cex.axis = 0.5, tck = 0)
# Add correlation values as text for highly correlated pairs
for(i in 1:nrow(correlation_matrix)) {
for(j in 1:ncol(correlation_matrix)) {
if (!is.na(correlation_matrix[i, j]) && i != j && abs(correlation_matrix[i, j]) > 0.6) {
text(j, i, labels = round(correlation_matrix[i, j], 2), cex = 0.5)
}
}
}
featureMatrix <- as.matrix(train[, -which(names(train) == "LOAN_DEFAULT")])
labelVector <- as.numeric(as.character(train$LOAN_DEFAULT))
# Set the parameters
params <- list(
objective = "binary:logistic",
eval_metric = "logloss",
max_depth = 6,
eta = 0.1,
nthread = 2,
subsample = 0.8,
colsample_bytree = 0.8
)
# Cross-validation for rounds
cvResults <- xgb.cv(
params = params,
data = featureMatrix,
label = labelVector,
nfold = 5,
nrounds = 100,
early_stopping_rounds = 10,
verbose = FALSE
)
# Train the XGBoost model
xgbModel <- xgboost(
params = params,
data = featureMatrix,
label = labelVector,
nrounds = cvResults$best_iteration
)
#Rank features
importanceMatrix <- xgb.importance(model = xgbModel)
#Modelling the features importance
importanceMatrix <- xgb.importance(model = xgbModel)
xgb.plot.importance(importanceMatrix)
# Sort the data by importance in descending order
importance_data <- importanceMatrix[order(importanceMatrix$Importance, decreasing = TRUE), ]
importance_data <- importance_data %>%
slice_head(n = 10)
# Create the bar plot
ggplot(importance_data, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Feature Importance") +
theme_classic() +
theme(axis.text.y = element_text(size = 8), axis.title.x=element_blank(), axis.title.y=element_blank()) +
coord_flip()
# Sort the data by importance in descending order
importance_data <- importanceMatrix[order(importanceMatrix$Importance, decreasing = TRUE), ]
importance_data <- importance_data %>%
slice_head(n = 15)
# Create the bar plot
ggplot(importance_data, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "skyblue") +
labs(title = "Feature Importance") +
theme_classic() +
theme(axis.text.y = element_text(size = 8), axis.title.x=element_blank(), axis.title.y=element_blank()) +
coord_flip()
#Extract log loss vector from Cross validation result
log_loss <- cvResults$evaluation_log$train_logloss_mean
#Create a data frame with log loss and number of features
logLossData <- data.frame(
NumFeatures = seq_along(log_loss),
LogLoss = log_loss)
#Plot log loss by number of features
ggplot(logLossData, aes(x = NumFeatures, y = LogLoss)) +
geom_line() +
geom_point() +
xlab("Number of Features") +
ylab("Log Loss") +
ggtitle("Log Loss by Number of Features") +
theme_bw()
#It appears that the optimal model should include 15 variables
# best_variables <- feature_importance$Feature[1:15]
# best_variables
topFeatures <- importanceMatrix$Feature[1:15]
print(importanceMatrix)
# Subset the dataset with the selected features
filteredData <- train[, c(topFeatures, "LOAN_DEFAULT")]
# Training/Testing split
set.seed(123)
n <- dim(filteredData)[1]
train_indices <- sample(1:n, size = round(0.8 * n))  # 80% for training
test_indices <- setdiff(1:n, train_indices)  # Remaining 20% for testing
train.data <- (filteredData[train_indices, 1:16])
test.data <- (filteredData[test_indices, 1:16])
train.labels <- as.numeric(filteredData$LOAN_DEFAULT[train_indices])
test.labels <- as.numeric(filteredData$LOAN_DEFAULT[test_indices])
#Verify for accuracy
head(train.data)
#Fit the Logistic regression on train data
logistic_model <- glm(LOAN_DEFAULT ~ ., data = train.data, family = binomial)
# Predictions on train data
y_pred_train <- predict(logistic_model, newdata = train.data, type = "response")
y_pred_class_train <- ifelse(y_pred_train > 0.5, 1, 0)
# Model evaluation on train data
train_accuracy <- mean(y_pred_class_train == train.labels)
train_confusion_matrix <- table(train.labels, y_pred_class_train, dnn = c("Actual", "Predicted"))
# Print accuracy and confusion matrix for train data
print(paste("Train Accuracy:", train_accuracy))
train_confusion_matrix
# Adjust the threshold to see if it improves the balance
threshold <- 0.4
y_pred_class_train_adjusted <- ifelse(y_pred_train > threshold, 1, 0)
train_confusion_matrix_adjusted <- table(train.labels, y_pred_class_train_adjusted, dnn = c("Actual", "Predicted"))
adjusted_train_accuracy <- mean(y_pred_class_train_adjusted == train.labels)
precision <- train_confusion_matrix_adjusted[2, 2] / sum(train_confusion_matrix_adjusted[, 2])
recall <- train_confusion_matrix_adjusted[2, 2] / sum(train_confusion_matrix_adjusted[2, ])
# Print adjusted accuracy, precision, and recall for train data
print(paste("Adjusted Train Accuracy:", adjusted_train_accuracy))
train_confusion_matrix_adjusted
print(paste("Precision:", precision))
print(paste("Recall:", recall))
#Matrix for explanatory variable
train.data <- as.matrix(train.data[ , 1:15])
test.data <- as.matrix(test.data[ , 1:15])
#str(train.data)
#str(test.data)
#Finding optimal eta with logloss in mind
etaTester <- seq(1, 0.01, by = -0.01)
logLossVal <- numeric(length(etaTester))
#iterating through model to see which one works the best. REAL SLOW ON HARDWARE
for(i in seq_along(etaTester)) {
xgb.loan <- xgboost(
data = train.data, label = train.labels,
max.depth = 6, eta = etaTester, nround = 100,
objective = "binary:logistic", eval_metric = "logloss",
verbose = 0
)
logLossVal[i] <- xgb.loan$evaluation_log$train_logloss[i]
}
bestIndex <- which.min(logLossVal)
bestEta <- etaTester[bestIndex]
bestLL <- logLossVal[bestIndex]
cat("Optimal eta: ", bestEta, "\n")
cat("Best logLoss: ", bestLL, "\n")
# XGBoost Classifier
xgb.loan = xgboost(data = train.data, label = train.labels,
max.depth = 6, eta = 0.01, nrounds = 100,
objective = "binary:logistic", eval_metric = "logloss")
# Early Stopping using cross-validation
xgb.loan.cv = xgb.cv(data = train.data, label = train.labels,
max.depth = 6, eta = 0.01, nrounds = 100,
nfold = 5, early_stopping_rounds = 5,
objective = "binary:logistic", eval_metric = "logloss")
sel_rounds = xgb.loan.cv$best_iteration
xgb.loan2 = xgboost(data = train.data, label = train.labels,
max.depth = 6, eta = 1, nrounds = sel_rounds,
objective = "binary:logistic", eval_metric = "logloss")
# Fitted response labels
pred1 = predict(xgb.loan, train.data)
pred.xgb.loan = ifelse(pred1 > 0.5, 2, 1)
pred2 = predict(xgb.loan2, train.data)
pred.xgb.loan2 = ifelse(pred2 > 0.5, 2, 1)
# Plotting the first two trees in the boosted classifier
xgb.plot.tree(model = xgb.loan, trees = 1:2)
# Misclassification errors in the training data
train_conf_matrix1 <- table(filteredData$LOAN_DEFAULT[train_indices], pred.xgb.loan)
train_conf_matrix2 <- table(filteredData$LOAN_DEFAULT[train_indices], pred.xgb.loan2)
#Accuracy
train_accuracy1 <- (train_conf_matrix1[1,1] + train_conf_matrix1[2,2]) / sum(train_conf_matrix1)
precision1 <- train_conf_matrix1[2, 2] / sum(train_conf_matrix1[, 2])
recall1 <- train_conf_matrix1[2, 2] / sum(train_conf_matrix1[2, ])
train_accuracy1
precision1
recall1
train_accuracy2 <- (train_conf_matrix2[1,1] + train_conf_matrix2[2,2]) / sum(train_conf_matrix2)
precision2 <- train_conf_matrix2[2, 2] / sum(train_conf_matrix2[, 2])
recall2 <- train_conf_matrix2[2, 2] / sum(train_conf_matrix2[2, ])
train_accuracy2
precision2
recall2
#training and Validation split
set.seed(4566)
n <- dim(filteredData)[1]
trainValInd <- sample(1:n, size = round(0.8 * n))
testIndices <- setdiff(1:n, trainValInd)
# Split the train_val_indices into training and validation sets
trainIndices <- sample(train_val_indices, size = round(0.75 * length(train_val_indices)))  # 80% - Training
#training and Validation split
set.seed(4566)
n <- dim(filteredData)[1]
trainValInd <- sample(1:n, size = round(0.8 * n))
testIndices <- setdiff(1:n, trainValInd)
# Split the train_val_indices into training and validation sets
trainIndices <- sample(trainValInd, size = round(0.8 * length(trainValInd)))  # 80% - Training
valIndices <- setdiff(trainValInd, trainIndices)  # 20% - validation
train2.data <- (filteredData[trainIndices, 1:16])
val.data <- (filteredData[valIndices, 1:16])
train2.labels <- as.numeric(filteredData$LOAN_DEFAULT[trainIndices])
val.labels <- as.numeric(filteredData$LOAN_DEFAULT[valIndices])
train2.data <- as.matrix(train.data[ , 1:15])
val.data <- as.matrix(val.data[ , 1:15])
xgb.loan3 <- xgboost(
data = train2.data, label = train.labels,
max.depth = 6, eta = 0.01, nrounds = 100, #left this as eta = 0.01 from the last time.
objective = "binary:logistic", eval_metric = "logloss",
verbose = 0
)
trainPredict2 <- predict(xgb.loan3, train.data2)
train2.data <- as.matrix(train.data[ , 1:15])
val.data <- as.matrix(val.data[ , 1:15])
xgb.loan3 <- xgboost(
data = train2.data, label = train.labels,
max.depth = 6, eta = 0.01, nrounds = 100, #left this as eta = 0.01 from the last time.
objective = "binary:logistic", eval_metric = "logloss",
verbose = 0
)
trainPredict2 <- predict(xgb.loan3, train2.data)
trainPredictClass2 <- ifelse(trainPredict2 > 0.5, 1, 0)
trainAccuracy2 <- mean(trainPredictClass2 == train2.labels)
cat("Training Accuracy on new classifier: ", trainAccuracy2)
#Matrix for explanatory variable
train.matrix <- as.matrix(train.data[ , 1:15])
test.matrix <- as.matrix(test.data[ , 1:15])
#Finding optimal eta with logloss in mind
etaTester <- seq(1, 0.01, by = -0.01)
logLossVal <- numeric(length(etaTester))
#iterating through model to see which one works the best. REAL SLOW ON HARDWARE
for(i in seq_along(etaTester)) {
xgb.loan <- xgboost(
data = train.matrix, label = train.labels,
max.depth = 6, eta = etaTester, nround = 100,
objective = "binary:logistic", eval_metric = "logloss",
verbose = 0
)
logLossVal[i] <- xgb.loan$evaluation_log$train_logloss[i]
}
bestIndex <- which.min(logLossVal)
bestEta <- etaTester[bestIndex]
bestLL <- logLossVal[bestIndex]
cat("Optimal eta: ", bestEta, "\n")
cat("Best logLoss: ", bestLL, "\n")
#Create a data frame with log loss and eta
eta <- data.frame(
eta = etaTester,
LogLoss = logLossVal)
#Plot log loss by eta
ggplot(eta, aes(x = eta, y = LogLoss)) +
geom_line() +
geom_point() +
xlab("eta tester") +
ylab("Log Loss") +
ggtitle("Log Loss by eta values") +
theme_bw()
